{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f051f1b8-7f4b-41d8-9152-9d042e0b1a4b",
   "metadata": {},
   "source": [
    "# Predicting Telco Customer Churn Using Random Forest and Gradient Boosting\n",
    "\n",
    "In this notebook, we will use ensemble machine learning methods – **Random Forest** and **Gradient Boosting** – to build predictive models for Telco Customer Churn. We will go through dataset exploration, preprocessing, model training, evaluation, and analysis. This project demonstrates how ensemble methods can improve prediction performance over individual models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960d14a0-e3f5-477b-adbe-19cfa2f251da",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "Customer churn is a critical problem for telecom companies. Accurately predicting churn offers actionable insights to improve customer retention and reduce costs. \n",
    "\n",
    "We use two popular ensemble methods in this project:\n",
    "\n",
    "- **Random Forest:** Combines multiple decision trees trained on random subsets of data and features (bagging) to reduce variance and avoid overfitting.\n",
    "- **Gradient Boosting:** Sequentially builds decision trees that each correct the errors of its predecessors, focusing on the misclassified examples.\n",
    "\n",
    "Both methods have advantages for handling complex and noisy datasets and are widely used in real-world predictive modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fd9612-a969-4ec9-9a0e-8b6a8dc1b1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
    "                             f1_score, roc_auc_score, confusion_matrix, classification_report, roc_curve)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# For SHAP analysis\n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a06e19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure plots\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
    "plt.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1f3fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This module is used to handle warning messages in Python\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc050ced-d7c8-4e60-95ea-0855d50e2122",
   "metadata": {},
   "source": [
    "## 2. Dataset Description & Exploratory Data Analysis (EDA)\n",
    "\n",
    "We use the Telco Customer Churn dataset. The dataset includes customers’ demographic, account, and service data along with whether they churned. Some of the key features include:\n",
    "\n",
    "- `gender`, `SeniorCitizen`, `Partner`, `Dependents`\n",
    "- `tenure`, `PhoneService`, `MultipleLines`, \n",
    "- `InternetService`, `OnlineSecurity`, `TechSupport`, etc.\n",
    "- `MonthlyCharges`, `TotalCharges`, and the target variable `Churn`\n",
    "\n",
    "Let’s load the dataset and perform some initial exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5960b965-2b2b-42f2-9c6f-1556e66a80ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset from a public URL\n",
    "data_url = \"https://huggingface.co/KawgKawgKawg/Telephone-Company-Churn-Classification-Model/raw/main/Telco-Customer-Churn.csv\"\n",
    "df = pd.read_csv(data_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba55f59",
   "metadata": {},
   "source": [
    "### Preview of the Dataset\n",
    "\n",
    "Let's look at the first few rows of the dataset to get a sense of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12d37e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic information\n",
    "print('Dataset shape:', df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd6c5ea-d8d8-4e1f-9d82-8d25da42c2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick overview of the dataset\n",
    "print('Dataset info:')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681d35f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate descriptive statistics for the DataFrame 'df', including all columns and data types\n",
    "print('\\nSummary statistics:')\n",
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1152a45e",
   "metadata": {},
   "source": [
    "### Categorical Feature Distributions\n",
    "\n",
    "Let's visualize the distribution of some key categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6800be02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distributions for several important categorical features\n",
    "categorical_features = ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'InternetService', 'Contract', 'PaymentMethod']\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(categorical_features):\n",
    "    sns.countplot(x=col, data=df, palette='colorblind', ax=axes[i])\n",
    "    axes[i].set_title(f'Distribution of {col}')\n",
    "    axes[i].set_xlabel(col)\n",
    "    axes[i].set_ylabel('Count')\n",
    "\n",
    "# Hide any unused subplots\n",
    "for j in range(len(categorical_features), len(axes)):\n",
    "    axes[j].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d6f3c3-d96d-4fb9-b95f-3ba9bbaa08b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the distribution of the target variable 'Churn'\n",
    "plt.figure()\n",
    "sns.countplot(x='Churn', data=df, palette='pastel')\n",
    "plt.title('Distribution of Churn')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61009605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of Monthly Charges\n",
    "plt.figure()\n",
    "sns.histplot(df['MonthlyCharges'], kde=True, color='blue')\n",
    "plt.title('Distribution of Monthly Charges')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4afbb9-01e6-4faa-ad93-26665ac31603",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing\n",
    "\n",
    "This section includes:\n",
    "\n",
    "- **Handling missing values:** Converting and cleaning the `TotalCharges` column.\n",
    "- **Encoding categorical variables:** Using one-hot encoding for non-numeric features.\n",
    "- **Feature scaling:** Scaling numerical features using `StandardScaler`.\n",
    "- **Train/Test split:** Dividing the data for training and evaluation.\n",
    "\n",
    "Let's preprocess the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8c9639-2fa3-4700-95da-07b3f143fe7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'TotalCharges' to numeric and drop rows with NaN values if any\n",
    "if 'TotalCharges' in df.columns:\n",
    "    df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\n",
    "    df = df.dropna(subset=['TotalCharges'])\n",
    "\n",
    "# Drop 'customerID' column since it doesn't doesn't have any meaningful information for prediction\n",
    "df.drop('customerID', axis=1, inplace=True)\n",
    "\n",
    "# Convert target variable 'Churn' to binary: Yes -> 1, No -> 0\n",
    "df['Churn'] = df['Churn'].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "# Check for remaining missing values\n",
    "print('Missing values in each column:')\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43f5e78-3ca4-4bb5-9a23-7f014c073cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify categorical and numerical columns\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns.drop('Churn')\n",
    "print('Categorical columns:', list(categorical_cols))\n",
    "print('Numerical columns:', list(numerical_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcc6e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding for categorical variables (excluding the target if present)\n",
    "df_encoded = pd.get_dummies(df, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "# Scale numerical features\n",
    "scaler = StandardScaler()\n",
    "df_encoded[numerical_cols] = scaler.fit_transform(df_encoded[numerical_cols])\n",
    "\n",
    "df_encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187ec9bd-d26f-4673-abf2-5a7dbd7e73e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into features (X) and target (y)\n",
    "X = df_encoded.drop('Churn', axis=1)\n",
    "y = df_encoded['Churn']\n",
    "\n",
    "# Split data into training and testing sets (80-20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print('Training set shape:', X_train.shape)\n",
    "print('Testing set shape:', X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa33d895-e8f5-4f56-ba92-3fee13097bde",
   "metadata": {},
   "source": [
    "## 4. Mathematical Explanation\n",
    "\n",
    "**Random Forest**:\n",
    "\n",
    "- An ensemble of decision trees built using the bagging technique. \n",
    "- Each tree is trained on a bootstrapped subset of data with random feature selection, which reduces variance and overfitting.\n",
    "- Final prediction is typically made by aggregating the predictions from individual trees (majority vote for classification).\n",
    "\n",
    "**Gradient Boosting**:\n",
    "\n",
    "- Builds models sequentially; each new tree attempts to correct errors made by the previous ensemble.\n",
    "- Uses gradient descent to minimize a loss function, thereby focusing on “difficult” cases.\n",
    "- Tends to have lower bias but may be more prone to overfitting if not properly regularized.\n",
    "\n",
    "**Key Differences:**\n",
    "\n",
    "- **Random Forest:** Parallel training, less sensitive to hyperparameters, good for reducing overfitting.\n",
    "- **Gradient Boosting:** Sequential training, often achieves higher accuracy but requires careful tuning of hyperparameters (learning rate, number of trees, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a45297-6af0-44f7-81b4-40f89d2d8f5c",
   "metadata": {},
   "source": [
    "## 5. Model Training & Evaluation\n",
    "\n",
    "We'll now train two models:\n",
    "\n",
    "- A **Random Forest Classifier**\n",
    "- A **Gradient Boosting Classifier**\n",
    "\n",
    "We will perform hyperparameter tuning using grid search (with a minimal grid for demonstration) and compare their performance using metrics such as accuracy, precision, recall, F1-score, and ROC-AUC.\n",
    "\n",
    "Let's get started with model training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939ed61d",
   "metadata": {},
   "source": [
    "### 5.1 Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26d9a13-7131-4abf-9e7a-3deaeeb5b17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a parameter grid\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [50, 100, 200, 300],\n",
    "    'max_depth': [None, 3, 5, 10, 20]\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV\n",
    "rf_grid = GridSearchCV(RandomForestClassifier(random_state=42), rf_param_grid, cv=3, scoring='roc_auc', n_jobs=-1)\n",
    "rf_grid.fit(X_train, y_train)\n",
    "\n",
    "print('Best parameters for Random Forest:', rf_grid.best_params_)\n",
    "\n",
    "# Best estimator from grid search\n",
    "rf_model = rf_grid.best_estimator_\n",
    "rf_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a330245d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test data\n",
    "rf_pred = rf_model.predict(X_test)\n",
    "rf_pred_proba = rf_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print('\\nRandom Forest Evaluation Metrics:')\n",
    "print('Accuracy:', accuracy_score(y_test, rf_pred))\n",
    "print('Precision:', precision_score(y_test, rf_pred))\n",
    "print('Recall:', recall_score(y_test, rf_pred))\n",
    "print('F1 Score:', f1_score(y_test, rf_pred))\n",
    "print('ROC-AUC:', roc_auc_score(y_test, rf_pred_proba))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151889db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Report\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(y_test, rf_pred, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d2a1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm_rf = confusion_matrix(y_test, rf_pred)\n",
    "sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Random Forest Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979d0443",
   "metadata": {},
   "source": [
    "### 5.2 Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c410457d-4a96-4799-ba82-c8da1fb03cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a parameter grid\n",
    "gb_param_grid = {\n",
    "    'n_estimators': [50, 100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'max_depth': [3, 5, 10, 20]\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV\n",
    "gb_grid = GridSearchCV(GradientBoostingClassifier(random_state=42), gb_param_grid, cv=3, scoring='roc_auc', n_jobs=-1)\n",
    "gb_grid.fit(X_train, y_train)\n",
    "\n",
    "print('Best parameters for Gradient Boosting:', gb_grid.best_params_)\n",
    "\n",
    "# Best estimator\n",
    "gb_model = gb_grid.best_estimator_\n",
    "gb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d0ce6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test data\n",
    "gb_pred = gb_model.predict(X_test)\n",
    "gb_pred_proba = gb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print('\\nGradient Boosting Evaluation Metrics:')\n",
    "print('Accuracy:', accuracy_score(y_test, gb_pred))\n",
    "print('Precision:', precision_score(y_test, gb_pred))\n",
    "print('Recall:', recall_score(y_test, gb_pred))\n",
    "print('F1 Score:', f1_score(y_test, gb_pred))\n",
    "print('ROC-AUC:', roc_auc_score(y_test, gb_pred_proba))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4eb446",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nClassification Report:')\n",
    "print(classification_report(y_test, gb_pred, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0104900e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm_gb = confusion_matrix(y_test, gb_pred)\n",
    "sns.heatmap(cm_gb, annot=True, fmt='d', cmap='Greens')\n",
    "plt.title('Gradient Boosting Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c651b34",
   "metadata": {},
   "source": [
    "### Model Comparison Table\n",
    "\n",
    "Let's compare the performance of Random Forest and Gradient Boosting side by side.\n",
    "\n",
    "> **Note:** The parameter grids used for hyperparameter tuning were:\n",
    "> - Random Forest: `n_estimators` = [50, 100, 200, 300], `max_depth` = [None, 3, 5, 10, 20] (20 combinations)\n",
    "> - Gradient Boosting: `n_estimators` = [50, 100, 200, 300], `learning_rate` = [0.01, 0.05, 0.1], `max_depth` = [3, 5, 10, 20] (48 combinations; `None` removed for compatibility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6773098a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect metrics for both models\n",
    "metrics = {\n",
    "    \"Model\": [\"Random Forest\", \"Gradient Boosting\"],\n",
    "    \"Accuracy\": [\n",
    "        accuracy_score(y_test, rf_pred),\n",
    "        accuracy_score(y_test, gb_pred)\n",
    "    ],\n",
    "    \"Precision\": [\n",
    "        precision_score(y_test, rf_pred, zero_division=0),\n",
    "        precision_score(y_test, gb_pred, zero_division=0)\n",
    "    ],\n",
    "    \"Recall\": [\n",
    "        recall_score(y_test, rf_pred, zero_division=0),\n",
    "        recall_score(y_test, gb_pred, zero_division=0)\n",
    "    ],\n",
    "    \"F1 Score\": [\n",
    "        f1_score(y_test, rf_pred, zero_division=0),\n",
    "        f1_score(y_test, gb_pred, zero_division=0)\n",
    "    ],\n",
    "    \"ROC-AUC\": [\n",
    "        roc_auc_score(y_test, rf_pred_proba),\n",
    "        roc_auc_score(y_test, gb_pred_proba)\n",
    "    ]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(metrics)\n",
    "comparison_df.set_index(\"Model\", inplace=True)\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86054313",
   "metadata": {},
   "source": [
    "### ROC Curve Comparison\n",
    "\n",
    "Let's compare the ROC curves of both models on the same plot for a direct visual comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc050c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ROC curves\n",
    "fpr_rf, tpr_rf, _ = roc_curve(y_test, rf_pred_proba)\n",
    "fpr_gb, tpr_gb, _ = roc_curve(y_test, gb_pred_proba)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr_rf, tpr_rf, label=f'Random Forest (AUC = {roc_auc_score(y_test, rf_pred_proba):.2f})', color='#0072B2')\n",
    "plt.plot(fpr_gb, tpr_gb, label=f'Gradient Boosting (AUC = {roc_auc_score(y_test, gb_pred_proba):.2f})', color='#D55E00')\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Chance')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve Comparison')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2a5a8d-340b-4726-9ea6-31e41a7f2a41",
   "metadata": {},
   "source": [
    "## 6. Model Analysis & Visualization\n",
    "\n",
    "In this section, we analyze the models using feature importance and SHAP values for model interpretability.\n",
    "\n",
    "- **Feature Importance:** We plot the (normalized) importance scores for both models.\n",
    "- **SHAP Values:** SHAP (SHapley Additive exPlanations) provides insights into how each feature contributes to individual predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b427b4",
   "metadata": {},
   "source": [
    "### SHAP Analysis for Model Interpretability\n",
    "\n",
    "We use SHAP (SHapley Additive exPlanations) to interpret our models. SHAP values help us understand how each feature contributes to the model's predictions for individual samples.\n",
    "\n",
    "- **Summary Plot (Bar):** Shows the average absolute SHAP value for each feature, indicating overall feature importance.\n",
    "- **Summary Plot (Dot):** Shows the distribution of SHAP values for each feature across samples, revealing both importance and the direction of impact.\n",
    "\n",
    "> **Note:** For binary classification, SHAP returns a single array of values. For multi-class, it returns a list of arrays (one per class). Here, we use the array directly for binary classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218929a4",
   "metadata": {},
   "source": [
    "Let's first plot the feature importances and SHAP Values from the Random Forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79a4ac1-a727-4f7f-b194-cf9b3b2bb3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance from the Random Forest model\n",
    "rf_importances = pd.Series(rf_model.feature_importances_, index=X.columns)\n",
    "rf_importances = rf_importances.sort_values(ascending=False)[:15]  # Top 15 features\n",
    "\n",
    "plt.figure()\n",
    "rf_importances.plot(kind='bar')\n",
    "plt.title('Top 15 Feature Importances from Random Forest')\n",
    "plt.ylabel('Importance Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c87908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the TreeExplainer\n",
    "rf_explainer = shap.TreeExplainer(rf_model)\n",
    "\n",
    "# Compute SHAP values for a subset of the test data\n",
    "rf_shap_values = rf_explainer.shap_values(X_test.iloc[:100])\n",
    "\n",
    "# Plot the SHAP summary plot\n",
    "shap.initjs()\n",
    "shap.summary_plot(rf_shap_values[1], X_test.iloc[:100], plot_type='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b25785",
   "metadata": {},
   "source": [
    "Plot the feature importances and SHAP Values from the Gradient Boosting model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a44a3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance from the Gradient Boosting model\n",
    "gb_importances = pd.Series(gb_model.feature_importances_, index=X.columns)\n",
    "gb_importances = gb_importances.sort_values(ascending=False)[:15]  # Top 15 features\n",
    "\n",
    "plt.figure()\n",
    "gb_importances.plot(kind='bar')\n",
    "plt.title('Top 15 Feature Importances from Gradient Boosting')\n",
    "plt.ylabel('Importance Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d354b2-0823-4b9f-9799-1f95cf1e4c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the TreeExplainer\n",
    "gb_explainer = shap.TreeExplainer(gb_model)\n",
    "\n",
    "# Compute SHAP values for a subset of the test data\n",
    "gb_shap_values = gb_explainer.shap_values(X_test.iloc[:100])\n",
    "\n",
    "# Plot the SHAP summary plot\n",
    "shap.initjs()\n",
    "shap.summary_plot(gb_shap_values, X_test.iloc[:100], plot_type='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93f042d-7fc5-47d2-9bfc-4017554dbc66",
   "metadata": {},
   "source": [
    "## 7. Discussion\n",
    "\n",
    "From the results above, we can note the following:\n",
    "\n",
    "- Both Random Forest and Gradient Boosting achieved good performance in predicting customer churn.\n",
    "- The ROC curves and evaluation metrics (accuracy, precision, recall, F1, ROC-AUC) offer insights into the trade-offs between the models. \n",
    "- Feature importance and the SHAP values help us understand which features most strongly impact the prediction. For instance, features related to monthly charges and tenure often prove to be important.\n",
    "\n",
    "While Gradient Boosting sometimes achieves slightly higher ROC-AUC, it can be more sensitive to hyperparameter choices and may require careful tuning. On the other hand, Random Forest tends to be more robust but might not capture all patterns as finely as boosting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc36e986-dce3-4ae8-9acb-ccaec6ccd2e3",
   "metadata": {},
   "source": [
    "## 8. Conclusion\n",
    "\n",
    "In this project, we:\n",
    "\n",
    "- Explored and preprocessed the Telco Customer Churn dataset.\n",
    "- Built and tuned predictive models using both Random Forest and Gradient Boosting.\n",
    "- Evaluated and compared their performance using multiple metrics and visualized the model interpretation results using feature importances and SHAP plots.\n",
    "\n",
    "The insights gained can help telecom companies to proactively identify and address factors that contribute to customer churn. Future work may include:\n",
    "\n",
    "- Further hyperparameter tuning and cross-validation.\n",
    "- Handling class imbalances through methods such as SMOTE.\n",
    "- Exploring additional ensemble methods or deep learning techniques.\n",
    "- Integrating the model into a real-time deployment pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe914e6-7da9-4acf-85f2-9da6ac89c4d3",
   "metadata": {},
   "source": [
    "## 9. Additional Considerations\n",
    "\n",
    "- **Handling Class Imbalance:** In real-world applications, further techniques (like SMOTE or class-weighting) might be needed if the dataset is highly imbalanced.\n",
    "- **Model Optimization:** Additional parameters and cross-validation strategies can be applied to get the best performance.\n",
    "- **Deployment:** The trained model can be deployed using REST APIs, cloud services, or integrated into business applications for real-time predictions.\n",
    "\n",
    "This notebook provides a framework that can be extended for other applications such as credit scoring or product recommendation systems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be651458",
   "metadata": {},
   "source": [
    "## 10. References\n",
    "\n",
    "1. [Random Forests](https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm) by Leo Breiman.\n",
    "2. [Gradient Boosting Machine](https://research.google/pubs/pub49317/) by Friedman, J. H.\n",
    "3. [Telco Customer Churn Dataset](https://huggingface.co/KawgKawgKawg/Telephone-Company-Churn-Classification-Model/raw/main/Telco-Customer-Churn.csv) - Public dataset available on Hugging Face.\n",
    "4. [SHAP Documentation](https://github.com/slundberg/shap) - For model interpretability."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
